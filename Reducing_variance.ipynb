{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reducing_variance.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FU0TYajRTt23"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidcpage/Imagenette-experiments/blob/master/Reducing_variance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QHvmGOBpYuT",
        "colab_type": "text"
      },
      "source": [
        "In the [previous notebook](https://github.com/davidcpage/Imagenette-experiments/blob/master/Imagenette_v2.ipynb), we demonstrated Imagenette training of an xresnet model based on  https://github.com/lessw2020/Ranger-Mish-ImageWoof-5 using fastai and fastai2 codebases. We implemented a faster dataloader using Nvidia DALI and made some changes to the fastai2 model so that it agrees with the v1 version from the repo.\n",
        "\n",
        "In today's notebook, we are going to focus on reducing the variance of the validation accuracy to make it easier to compare training setups. The baseline 5 epoch xresnet18 training from last time achieves a mean Imagenette validation accuracy of around 88.3% with a std dev of about 0.7%. Our plan is to move some examples from the training set to make a larger validation set and to experiment with a smoothed version of the 0-1 accuracy metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU0TYajRTt23",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZSzbQ9Ag47l",
        "colab_type": "text"
      },
      "source": [
        "Install fastai2 and DALI. You may need to restart afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLGuUwFpLzc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m pip install typeguard\n",
        "!python -m pip install --upgrade pillow fastprogress\n",
        "!python -m pip install git+https://github.com/fastai/fastai2\n",
        "\n",
        "!python -m pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0 nvidia-dali\n",
        "\n",
        "RANGER = 'https://raw.githubusercontent.com/lessw2020/Ranger-Mish-ImageWoof-5/master/ranger.py'\n",
        "MXRESNET = 'https://raw.githubusercontent.com/lessw2020/Ranger-Mish-ImageWoof-5/master/mxresnet.py'\n",
        "UTILS = 'https://raw.githubusercontent.com/davidcpage/Imagenette-experiments/master/utils.py'\n",
        "\n",
        "!wget $RANGER -O ranger.py\n",
        "!wget $MXRESNET -O mxresnet.py\n",
        "!wget $UTILS -O utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7c9ywHZhNiT",
        "colab_type": "text"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch4lAVe4quvT",
        "colab_type": "text"
      },
      "source": [
        "Imports, device setup and dataset download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbkdYs8IM37W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import fastai, fastai.vision\n",
        "import fastai2, fastai2.callback.all\n",
        "\n",
        "import ranger\n",
        "\n",
        "data_dir = fastai.datasets.untar_data(fastai.datasets.URLs.IMAGENETTE_320)\n",
        "device = torch.device(torch.cuda.current_device())\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux2FJO34g--E",
        "colab_type": "text"
      },
      "source": [
        " We have moved the main functionality implemented last time to a script to reuse here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XAxQJUoj5RE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import DALIDataLoader, Chain, MockV1DataBunch, imagenet_train_graph, imagenet_valid_graph, fit_flat_cos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB2jM7pKk6UT",
        "colab_type": "text"
      },
      "source": [
        "Let's use this to build fastai and fastai2 compatible (DALI) dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wO66U5HPRvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = 128\n",
        "bs = 64\n",
        "random_aspect_ratio = (3/4, 4/3)\n",
        "random_area = (0.35, 1.)\n",
        "val_xtra_size = 32\n",
        "\n",
        "train_dl = lambda folder, bs, seed=-1: (\n",
        "        DALIDataLoader(imagenet_train_graph(folder, size, random_aspect_ratio, random_area), bs, drop_last=True, device=device, seed=seed))\n",
        "valid_dl = lambda folder, bs, : Chain(\n",
        "        DALIDataLoader(imagenet_valid_graph(folder, size, val_xtra_size), bs, drop_last=False, device=device),\n",
        "        DALIDataLoader(imagenet_valid_graph(folder, size, val_xtra_size, mirror=1), bs, drop_last=False, device=device),\n",
        "    )\n",
        "\n",
        "data_v1 = lambda data_dir=data_dir, bs=bs: MockV1DataBunch(train_dl(data_dir/'train', bs), valid_dl(data_dir/'val', bs))\n",
        "data_v2 = lambda data_dir=data_dir, bs=bs: fastai2.basics.DataBunch(train_dl(data_dir/'train', bs), valid_dl(data_dir/'val', bs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9WT8dkBlw_T",
        "colab_type": "text"
      },
      "source": [
        "Let's recap by comparing a simple training run using the v1 and v2 codebases. The v2 version is basically a repeat of what we had at the end of last time and we expect that there are still differences from v1. Since we tied out the model between codebases, we are going to use the more flexible v2 model from now on even for v1 training. For what it's worth, we've also implemented a slightly faster version of the Mish activation function (compared to the v1 version and the jitted version in v2.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2SvJ_XkldvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import xresnet18, MishJit\n",
        "model = partial(xresnet18, c_out=10, sa=1, sym=0, act_cls=MishJit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJcyvpVXr_pz",
        "colab_type": "text"
      },
      "source": [
        "v1 training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbwxPTvcRgG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f72d1126-b2c3-44d6-b618-91b76043bb05"
      },
      "source": [
        "learner_v1 = partial(\n",
        "    fastai.basic_train.Learner, wd=1e-2, bn_wd=False, true_wd=True,\n",
        "    opt_func=partial(ranger.Ranger, betas=(0.95, 0.99), eps=1e-6),\n",
        "    metrics=(fastai.metrics.accuracy,),\n",
        "    loss_func=fastai.layers.LabelSmoothingCrossEntropy())\n",
        "\n",
        "learn = fit_flat_cos(learner_v1(data_v1(), model()).to_fp16(), n_epoch=5, lr=4e-3, pct_start=0.72)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.412312</td>\n",
              "      <td>1.486379</td>\n",
              "      <td>0.605000</td>\n",
              "      <td>00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.196931</td>\n",
              "      <td>1.027858</td>\n",
              "      <td>0.810000</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.106371</td>\n",
              "      <td>0.979379</td>\n",
              "      <td>0.807000</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.016692</td>\n",
              "      <td>0.896012</td>\n",
              "      <td>0.856000</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.906713</td>\n",
              "      <td>0.838668</td>\n",
              "      <td>0.884000</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LVf_rp1sCpy",
        "colab_type": "text"
      },
      "source": [
        "v2 training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpF1Vk0XnBCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d37c4c34-3fcf-4565-8d2e-6f4eeb480775"
      },
      "source": [
        "RangerWrapper = lambda *args, **kwargs: fastai2.basics.OptimWrapper(ranger.Ranger(*args, **kwargs))\n",
        "learner_v2 = partial(\n",
        "    fastai2.basics.Learner, lr=4e-3,\n",
        "    opt_func=partial(RangerWrapper, betas=(0.95, 0.99), eps=1e-6),\n",
        "    metrics=(fastai2.metrics.accuracy,),\n",
        "    loss_func=fastai2.basics.LabelSmoothingCrossEntropy())\n",
        "\n",
        "learn = learner_v2(data_v2(), model()).to_fp16().fit_flat_cos(n_epoch=5, wd=1e-2, pct_start=0.72)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.422901</td>\n",
              "      <td>1.150089</td>\n",
              "      <td>0.758000</td>\n",
              "      <td>00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.191645</td>\n",
              "      <td>1.050608</td>\n",
              "      <td>0.781000</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.095837</td>\n",
              "      <td>1.194106</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.024172</td>\n",
              "      <td>1.012769</td>\n",
              "      <td>0.819000</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.912284</td>\n",
              "      <td>0.841720</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjhAZ1asTSU",
        "colab_type": "text"
      },
      "source": [
        "Note that training takes a couple of seconds more per epoch on fastai2, which needs investigating at some point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdN17Rjjs0Ao",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrfItuYByAq9",
        "colab_type": "text"
      },
      "source": [
        "Here is a utility to make a new data set split with 250 examples per class instead of the original 50. These are moved from the training set so the new training set has 10894 examples from the original 12894."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7CYZ6dIRRzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "listdir = lambda dir: sorted(os.listdir(dir)) #deterministic ordering...\n",
        "\n",
        "def make_new_split(data_dir, new_data_dir, val_examples_per_class=250, seed=1234):\n",
        "    #keep the original 50 validation examples in each class\n",
        "    #and move over 'val_examples_per_class'-50 more from the train set\n",
        "    new_data_dir = Path(new_data_dir)\n",
        "    num_move = val_examples_per_class - 50  \n",
        "    assert num_move > 0 \n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    shutil.copytree(data_dir, new_data_dir)\n",
        "    train_dir, val_dir = Path(new_data_dir)/'train', Path(new_data_dir)/'val'\n",
        "    for k in listdir(train_dir):\n",
        "        files = listdir(train_dir/k)\n",
        "        for f in rng.choice(files, num_move, replace=False):\n",
        "            shutil.move(str(train_dir/k/f), val_dir/k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9fZHi7Ct8NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data_dir = Path(str(data_dir)+'-new')\n",
        "if not new_data_dir.exists(): \n",
        "    make_new_split(data_dir, new_data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJUIBcPuu9W9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "de98bb22-8ead-46f7-88f8-61bda2945a34"
      },
      "source": [
        "learn = fit_flat_cos(learner_v1(data_v1(new_data_dir), model()).to_fp16(), n_epoch=5, lr=4e-3, pct_start=0.72)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.517955</td>\n",
              "      <td>1.452000</td>\n",
              "      <td>0.629800</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.269426</td>\n",
              "      <td>1.308483</td>\n",
              "      <td>0.672400</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.153134</td>\n",
              "      <td>1.090687</td>\n",
              "      <td>0.759200</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.087146</td>\n",
              "      <td>1.029770</td>\n",
              "      <td>0.793200</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.958357</td>\n",
              "      <td>0.953012</td>\n",
              "      <td>0.825200</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nvlzjHqzWsd",
        "colab_type": "text"
      },
      "source": [
        "Validation accuracy is reduced by 4-5% which is not surprising since the new training set is smaller and the new validation set may have different difficulty just by chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16UMaKJw0vfH",
        "colab_type": "text"
      },
      "source": [
        "### Smoothed accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbGEjYPL0xhF",
        "colab_type": "text"
      },
      "source": [
        "Before we launch a set of training runs to test the variance of validation accuracy for the new dataset, let's try one more thing. Validation noise comes largely from examples on which the model is not sure which class to predict. Small changes in output class probabilities can lead to a change of predicted (argmax) class and thus model accuracy. The situation is improved with a larger validaton set to average over the noise, but we can potentially improve things further by smoothing the decision boundary using a soft(arg)max. \n",
        "\n",
        "This is a typical bias/variance tradeoff where we can reduce the variance of the accuracy metric at the expense of introducing a controlled amount of bias. It is likely that the bias more-or-less cancels out when we compare similar training settings in which case the reduction in variance would be a net win. In any case, it is cheap to add such a smoothed accuracy as an additional validation metric and we can decide if we want to use it later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCB2MISq0J-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smoothed_acc(logits, targets, beta=3.): #replace argmax with soft(arg)max\n",
        "    return torch.mean(nn.functional.softmax(logits*beta, dim=-1)[torch.arange(0, targets.size(0), device=device), targets])\n",
        "\n",
        "metrics = [fastai.metrics.accuracy, smoothed_acc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fetvtvq3abj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7510c391-a311-4386-9ee5-a81df2aa2fd0"
      },
      "source": [
        "learn = fit_flat_cos(learner_v1(data_v1(new_data_dir), model(), metrics=metrics).to_fp16(), n_epoch=5, lr=4e-3, pct_start=0.72)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>smoothed_acc</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.489646</td>\n",
              "      <td>1.520813</td>\n",
              "      <td>0.596200</td>\n",
              "      <td>0.578618</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.255452</td>\n",
              "      <td>1.317987</td>\n",
              "      <td>0.665400</td>\n",
              "      <td>0.644284</td>\n",
              "      <td>00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.162168</td>\n",
              "      <td>1.093264</td>\n",
              "      <td>0.764800</td>\n",
              "      <td>0.742068</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.081428</td>\n",
              "      <td>1.028166</td>\n",
              "      <td>0.791600</td>\n",
              "      <td>0.777946</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.954942</td>\n",
              "      <td>0.931859</td>\n",
              "      <td>0.835800</td>\n",
              "      <td>0.822356</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL6ceq7S3-zO",
        "colab_type": "text"
      },
      "source": [
        "The smoothed accuracy metric appears to be about 1.5% lower than the true 0-1 accuracy. Let's launch a set of training runs to measure the variance of the validation accuracy for our new dataset and to see if the smoothed accuracy metric improves things further:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW4w9ggR3nal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = (fit_flat_cos(learner_v1(data_v1(new_data_dir), model(), metrics=metrics).to_fp16(), n_epoch=5, lr=4e-3, pct_start=0.72) for _ in range(10))\n",
        "acc, acc_smoothed = zip(*[[x.item() for x in learn.recorder.metrics[-1]] for learn in results])\n",
        "np.mean(acc), np.std(acc), np.mean(acc_smoothed), np.std(acc_smoothed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27iemGoNE57N",
        "colab_type": "text"
      },
      "source": [
        "The new dataset has a validation accuracy of ~83.5% with a std dev of ~0.35% whilst the smoothed accuracy is ~82.1% with a std dev of ~0.25%. Note that the smoothed accuracy is just an alternative metric and doesn't affect training. It remains to be seen whether the bias that this introduces is consistent across different training settings and/or whether a different value of beta would be more appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCaTSRlc81i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}