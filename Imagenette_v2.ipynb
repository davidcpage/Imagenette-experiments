{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imagenette_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidcpage/Imagenette-experiments/blob/master/Imagenette_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8mLtXI5gfGV",
        "colab_type": "text"
      },
      "source": [
        "The aim of this notebook is to convert the Imagenette/woof training examples from https://github.com/lessw2020/Ranger-Mish-ImageWoof-5 to the new fastai v2 codebase. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU0TYajRTt23",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZSzbQ9Ag47l",
        "colab_type": "text"
      },
      "source": [
        "Install fastai_dev (aka v2) You may need to restart after installing pillow to pick up the new version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLGuUwFpLzc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m pip install typeguard\n",
        "!python -m pip install --upgrade pillow\n",
        "!git clone -q https://github.com/fastai/fastai_dev.git\n",
        "\n",
        "RANGER = 'https://raw.githubusercontent.com/lessw2020/Ranger-Deep-Learning-Optimizer/master/ranger.py'\n",
        "MXRESNET = 'https://raw.githubusercontent.com/lessw2020/Ranger-Mish-ImageWoof-5/master/mxresnet.py'\n",
        "!wget -q $RANGER -O fastai_dev/dev/ranger.py\n",
        "!wget -q $MXRESNET -O fastai_dev/dev/mxresnet.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Eqi9AJotZP",
        "colab_type": "text"
      },
      "source": [
        "Install Nvidia DALI, which we use for fast dataloading/augmentation below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNLfACyOoq7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0 nvidia-dali"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7c9ywHZhNiT",
        "colab_type": "text"
      },
      "source": [
        "Change to the fastai_dev directory, basic imports and device setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbkdYs8IM37W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "544f050f-8975-4ad2-dca1-23f4aa002b16"
      },
      "source": [
        "%cd fastai_dev/dev\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(torch.cuda.current_device())\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/jupyter/fastai_dev/dev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvQYKhXLhr8I",
        "colab_type": "text"
      },
      "source": [
        "#### Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wO66U5HPRvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = 128\n",
        "bs = 64\n",
        "\n",
        "random_aspect_ratio = (3/4, 4/3)\n",
        "random_area = (0.35, 1.)\n",
        "val_xtra_size = 32 \n",
        "interpolation = 2 #FIXME: what is this?\n",
        "\n",
        "data_url = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFzsy_gcNlxU",
        "colab_type": "text"
      },
      "source": [
        "### Fastai v1 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRe8tInlNvmR",
        "colab_type": "text"
      },
      "source": [
        "First let's establish a baseline on the v1 codebase. The code here is essentially taken from https://github.com/lessw2020/Ranger-Mish-ImageWoof-5/blob/master/train.py. \n",
        "\n",
        "**NB:** throughout the notebook we use fully qualified names for imported functions to avoid name-clashes between the two fastai codebases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFpzkgp4PGz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13f6650d-a720-4785-a0c8-5ac283cee3fb"
      },
      "source": [
        "import fastai.datasets, fastai.vision, fastai.vision.data, fastai.basic_train\n",
        "import fastai.metrics, fastai.layers, fastai.callbacks\n",
        "\n",
        "import ranger\n",
        "import mxresnet"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mish activation loaded...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqtpNQMBN-4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source = fastai.datasets.untar_data(data_url)\n",
        "\n",
        "data = (fastai.vision.data.ImageList.from_folder(source).split_by_folder(valid='val')\n",
        "            .label_from_folder().transform(([fastai.vision.flip_lr(p=0.5)], []), size=size)\n",
        "            .databunch(bs=bs, num_workers=4)\n",
        "            .presize(size, scale=random_area, ratio=random_aspect_ratio, val_xtra_size=val_xtra_size, interpolation=interpolation)\n",
        "            .normalize(fastai.vision.imagenet_stats))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbwxPTvcRgG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_then_cosine_sched(learn, n_batch, lr, annealing_start):\n",
        "    return fastai.callbacks.GeneralScheduler(learn, phases=[\n",
        "        fastai.callbacks.TrainingPhase(annealing_start*n_batch).schedule_hp('lr', lr),\n",
        "        fastai.callbacks.TrainingPhase((1-annealing_start)*n_batch).schedule_hp('lr', lr, anneal=fastai.callback.annealing_cos)     \n",
        "    ])\n",
        "\n",
        "def train(\n",
        "    data, \n",
        "    num_epoch=5,\n",
        "    model_func=partial(mxresnet.mxresnet50, c_out=10, sa=1, sym=0),\n",
        "    opt_func=partial(ranger.Ranger, betas=(0.95, 0.99), eps=1e-6),\n",
        "    lr_schedule=partial(flat_then_cosine_sched, lr=4e-3, annealing_start=0.72),\n",
        "    wd=1e-2,\n",
        "):\n",
        "    learn = fastai.basic_train.Learner(data, model_func(), wd=wd, opt_func=opt_func,\n",
        "                metrics=[fastai.metrics.accuracy],\n",
        "                bn_wd=False, true_wd=True,\n",
        "                loss_func = fastai.layers.LabelSmoothingCrossEntropy(),\n",
        "                callback_fns=[]).to_fp16(dynamic=True)\n",
        "    learn.fit(num_epoch, callbacks=[\n",
        "        lr_schedule(learn, n_batch=len(learn.data.train_dl) * num_epoch)])\n",
        "    return learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6s8x8vriI4Q",
        "colab_type": "text"
      },
      "source": [
        "In order to keep things relatively fast, let's train an mxresnet18 model for 5 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oblUiPxmUXci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "15fdb712-4d26-468b-d89b-e106eaf32b5f"
      },
      "source": [
        "learn = train(data, 5, model_func=partial(mxresnet.mxresnet18, c_out=10, sa=1, sym=0))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.442921</td>\n",
              "      <td>1.328564</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.207998</td>\n",
              "      <td>1.043967</td>\n",
              "      <td>0.802000</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.109578</td>\n",
              "      <td>1.150266</td>\n",
              "      <td>0.736000</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.045805</td>\n",
              "      <td>0.926282</td>\n",
              "      <td>0.836000</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.915382</td>\n",
              "      <td>0.844332</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ASahDA1VOw8",
        "colab_type": "text"
      },
      "source": [
        "### Nvidia DALI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPojLp2SjYLO",
        "colab_type": "text"
      },
      "source": [
        "We can speed things up (especially for small models) by using Nvidia DALI to do the dataloading/augmentation. For now we will use this for both fastai v1 and v2 models although we will want to test v2 dataloading at the end. The details of the code are not very interesting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sq6NHHIVQpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nvidia.dali.pipeline import Pipeline\n",
        "import nvidia.dali.ops as ops\n",
        "import nvidia.dali.types as types\n",
        "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
        "import math\n",
        "\n",
        "class _Pipe(Pipeline):\n",
        "    def __init__(self, graph, batch_size, num_threads, device_id, seed):\n",
        "        super().__init__(batch_size, num_threads, device_id, seed=seed)\n",
        "        self.define_graph = graph\n",
        "        self.build()\n",
        "\n",
        "class DALIDataLoader():\n",
        "    def __init__(self, graph, batch_size, drop_last=False, num_threads=4, device=None, seed=-1):\n",
        "        self.device = device\n",
        "        self.pipe = _Pipe(graph, batch_size, num_threads=num_threads, device_id=self.device.index, seed=seed)\n",
        "        n = self.pipe.epoch_size('Reader')\n",
        "        if drop_last:\n",
        "            self.length = n // batch_size\n",
        "            n = self.length * batch_size \n",
        "        else:\n",
        "            self.length = math.ceil(n//batch_size)\n",
        "        self.dali_iter = DALIGenericIterator([self.pipe], ['data', 'label'], n, auto_reset=True, fill_last_batch=False)\n",
        "\n",
        "    def __iter__(self): return ((batch[0]['data'], \n",
        "                                 batch[0]['label'].squeeze().to(dtype=torch.int64, device=self.device)\n",
        "                                ) for batch in self.dali_iter)\n",
        "    \n",
        "    def __len__(self): return self.length\n",
        "\n",
        "def train_graph(data_dir, size, random_aspect_ratio, random_area, interp_type,\n",
        "                stats=fastai.vision.imagenet_stats):\n",
        "    inputs = ops.FileReader(file_root=data_dir, random_shuffle=True)\n",
        "    decode = ops.ImageDecoderRandomCrop(\n",
        "            device='mixed', output_type=types.RGB, \n",
        "            random_aspect_ratio=random_aspect_ratio, random_area=random_area, num_attempts=100)\n",
        "    resize = ops.Resize(device='gpu', resize_x=size, resize_y=size, \n",
        "                        interp_type=interp_type)\n",
        "    mean, std = [[x*255 for x in stat] for stat in stats]\n",
        "    crop_mirror_norm = ops.CropMirrorNormalize(\n",
        "                        device='gpu', output_dtype=types.FLOAT16, output_layout=types.NCHW, \n",
        "                        crop=(size, size), image_type=types.RGB, mean=mean, std=std)\n",
        "    coin = ops.CoinFlip(probability=0.5)\n",
        "\n",
        "    def define_graph():    \n",
        "        jpegs, labels = inputs(name='Reader')\n",
        "        output = crop_mirror_norm(resize(decode(jpegs)), mirror=coin())\n",
        "        return [output, labels]\n",
        "    return define_graph\n",
        "\n",
        "def valid_graph(data_dir, size, val_xtra_size, interp_type, stats=fastai.vision.imagenet_stats, mirror=0):\n",
        "    inputs = ops.FileReader(file_root=data_dir, random_shuffle=False)\n",
        "    decode = ops.ImageDecoder(device='mixed', output_type=types.RGB)\n",
        "    resize = ops.Resize(device='gpu', resize_shorter=size+val_xtra_size, \n",
        "                        interp_type=interp_type)\n",
        "    mean, std = [[x*255 for x in stat] for stat in stats]\n",
        "    crop_mirror_norm = ops.CropMirrorNormalize(\n",
        "                        device='gpu', output_dtype=types.FLOAT16, output_layout=types.NCHW, \n",
        "                        crop=(size, size), image_type=types.RGB, mean=mean, std=std, mirror=mirror)\n",
        "    def define_graph():\n",
        "        jpegs, labels = inputs(name='Reader')\n",
        "        output = crop_mirror_norm(resize(decode(jpegs)))\n",
        "        return [output, labels]\n",
        "    return define_graph\n",
        "\n",
        "class MockV1DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, path='dummy', empty_val=False):\n",
        "        self.train_dl = train_dl\n",
        "        if not hasattr(train_dl, 'dataset'): train_dl.dataset = 'dummy'\n",
        "        self.valid_dl = valid_dl\n",
        "        if not hasattr(valid_dl, 'dataset'): valid_dl.dataset = 'dummy'\n",
        "        self.path = path\n",
        "        self.device = train_dl.device\n",
        "        self.empty_val = empty_val\n",
        "    def add_tfm(self, tfm): pass\n",
        "    def remove_tfm(self, tfm): pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km5FiFyDkmwJ",
        "colab_type": "text"
      },
      "source": [
        "The validation set of Imagenette is tiny, consisting of 500 examples only. This leads to substantial noise in the validation accuracies. To help a little, we are going to concatenate a left-right flipped version onto to the validation set to get an effective 1000 examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ZFAN2lVQjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp_type = types.INTERP_TRIANGULAR\n",
        "\n",
        "from itertools import chain\n",
        "class Chain():\n",
        "    def __init__(self, *dls):\n",
        "        self.dls = dls\n",
        "        self.device = self.dls[0].device\n",
        "    def __iter__(self): return chain(*self.dls)\n",
        "    def __len__(self): return sum(len(dl) for dl in self.dls)\n",
        "\n",
        "train_dl = lambda seed=-1: DALIDataLoader(train_graph(source/'train', size, random_aspect_ratio, random_area, interp_type), bs, drop_last=True, device=device, seed=seed)\n",
        "valid_dl = lambda: Chain(\n",
        "        DALIDataLoader(valid_graph(source/'val', size, val_xtra_size, interp_type), bs, drop_last=False, device=device),\n",
        "        DALIDataLoader(valid_graph(source/'val', size, val_xtra_size, interp_type, mirror=1), bs, drop_last=False, device=device),\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKV_fGfmk6f2",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to test training on the v1 codebase with DALI dataloading:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh3OwOfRTUB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5bfee210-7277-461b-9746-923a98474f99"
      },
      "source": [
        "dali_data = MockV1DataBunch(train_dl(), valid_dl())\n",
        "learn = train(dali_data, 5, model_func=partial(mxresnet.mxresnet18, c_out=10, sa=1, sym=0))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.465935</td>\n",
              "      <td>1.290174</td>\n",
              "      <td>0.696833</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.205533</td>\n",
              "      <td>0.995054</td>\n",
              "      <td>0.813406</td>\n",
              "      <td>00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.100494</td>\n",
              "      <td>1.030867</td>\n",
              "      <td>0.805430</td>\n",
              "      <td>00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.036184</td>\n",
              "      <td>0.904659</td>\n",
              "      <td>0.846014</td>\n",
              "      <td>00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.915965</td>\n",
              "      <td>0.843804</td>\n",
              "      <td>0.877828</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTVevmh6lyPF",
        "colab_type": "text"
      },
      "source": [
        "The training is substantially faster (at least on a GCP T4 GPU with 4 cpu cores) and accuracy is similar (although noise is still very large.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um6YrUVZkNpA",
        "colab_type": "text"
      },
      "source": [
        "### Fastai v2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_cwvCkprxH5",
        "colab_type": "text"
      },
      "source": [
        "Next we want to compare a model using the v2 codebase to the v1 model above. Here is the v1 model again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjMFCkw2r8qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_v1 = partial(mxresnet.mxresnet18, c_out=10, sa=1, sym=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCa6wKrZr_M8",
        "colab_type": "text"
      },
      "source": [
        "A similar model is available in v2. To start with let's use the Mish activation class from the v1 model to minimise differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN-bLy6skieq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "969fa4c0-6251-4816-f2ad-64b2f3962add"
      },
      "source": [
        "import local.vision.models.xresnet\n",
        "mish = mxresnet.Mish()\n",
        "model_v2 = partial(local.vision.models.xresnet.xresnet18, c_out=10, sa=1, sym=0, act_cls=(lambda: mish))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mish activation loaded...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTYAAEhqyiqg",
        "colab_type": "text"
      },
      "source": [
        "First let's compare the types of modules in the two models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1-KlD0q3ia5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b68c8173-854a-4e72-af77-b826e1b1db68"
      },
      "source": [
        "s1 = set(type(x) for x in model_v1().modules())\n",
        "s2 = set(type(x) for x in model_v2().modules())\n",
        "s1^s2 #symmetric difference"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{local.layers.ConvLayer,\n",
              " local.layers.Flatten,\n",
              " local.layers.ResBlock,\n",
              " local.vision.models.xresnet.XResNet,\n",
              " mxresnet.Flatten,\n",
              " mxresnet.MXResNet,\n",
              " mxresnet.ResBlock,\n",
              " mxresnet.SimpleSelfAttention,\n",
              " torch.nn.modules.activation.ReLU,\n",
              " torch.nn.modules.conv.Conv1d}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf9GH9w76jGv",
        "colab_type": "text"
      },
      "source": [
        "Some of these are implementation specific compound layers. Let's ignore them for now and we will come back to them later if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wQRWkvX6iLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "types_to_ignore = {\n",
        "    torch.nn.Sequential,\n",
        "\n",
        "    mxresnet.Flatten, \n",
        "    mxresnet.MXResNet, \n",
        "    mxresnet.ResBlock, \n",
        "\n",
        "    local.layers.Flatten, \n",
        "    local.vision.models.xresnet.XResNet,\n",
        "    local.layers.ResBlock, \n",
        "    local.layers.ConvLayer, \n",
        "}\n",
        "\n",
        "filtered_modules = lambda model: (x for x in model.modules() if type(x) not in types_to_ignore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuM3Jzit-8oy",
        "colab_type": "text"
      },
      "source": [
        "Now let's compare the filtered modules in a bit more detail by comparing `repr` strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to1eMrVV4MeW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "41cb534b-74e3-4426-c879-28d727302103"
      },
      "source": [
        "s1 = set(repr(x) for x in filtered_modules(model_v1()))\n",
        "s2 = set(repr(x) for x in filtered_modules(model_v2()))\n",
        "s1 ^ s2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)',\n",
              " 'Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)',\n",
              " 'ReLU()',\n",
              " 'SimpleSelfAttention(\\n  (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\\n)'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysFrMZH-_mTU",
        "colab_type": "text"
      },
      "source": [
        "Let's locate the offending modules in the v1 model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pjqeCLK-PVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ddfb3172-9d01-430e-fe59-db33e2f98ea3"
      },
      "source": [
        "{k: x for k,x in model_v1().named_modules() if repr(x) in s1^s2}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'4.1.sa': SimpleSelfAttention(\n",
              "   (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
              " ), '4.1.sa.conv': Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AMO0MgE_q4C",
        "colab_type": "text"
      },
      "source": [
        "and in the v2 model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrPwAmWU-hjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9047d746-3cfe-4ca1-cb62-9778e3890762"
      },
      "source": [
        "{k: x for k,x in model_v2().named_modules() if repr(x) in s1^s2}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.2': ReLU(),\n",
              " '1.0': Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
              " '1.2': ReLU(),\n",
              " '2.2': ReLU()}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yuhTB3pACPX",
        "colab_type": "text"
      },
      "source": [
        "The v1 model has a SimpleSelfAttention layer which is missing from the v2 model, whilst the v2 model still has some ReLU activations at early layers and one Conv layer has a different shape. Let's fix these issues with a modified v2 model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl645qlZ_tHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XResNet(nn.Sequential):\n",
        "    def __init__(self, expansion, layers, c_in=3, c_out=1000, \n",
        "                 sa=False, sym=False, act_cls=local.basics.defaults.activation,\n",
        "                 ):\n",
        "        stem = []\n",
        "        sizes = [c_in, 16,32,64] if c_in < 3 else [c_in, 32, 64, 64] \n",
        "        for i in range(3):\n",
        "            stem.append(local.layers.ConvLayer(sizes[i], sizes[i+1], stride=2 if i==0 else 1, act_cls=act_cls))\n",
        "\n",
        "        block_szs = [64//expansion,64,128,256,512] +[256]*(len(layers)-4)\n",
        "        blocks = [self._make_layer(expansion, ni=block_szs[i], nf=block_szs[i+1], blocks=l, stride=1 if i==0 else 2,\n",
        "                                  sa=sa if i==len(layers)-4 else False, sym=sym, act_cls=act_cls)\n",
        "                  for i,l in enumerate(layers)]\n",
        "        super().__init__(\n",
        "            *stem,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            *blocks,\n",
        "            nn.AdaptiveAvgPool2d(1), local.layers.Flatten(),\n",
        "            nn.Linear(block_szs[-1]*expansion, c_out),\n",
        "        )\n",
        "        local.vision.models.xresnet.init_cnn(self)\n",
        "\n",
        "    def _make_layer(self, expansion, ni, nf, blocks, stride, sa, sym, act_cls):\n",
        "        return nn.Sequential(\n",
        "            *[local.layers.ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1,\n",
        "                      sa if i==(blocks-1) else False, sym=sym, act_cls=act_cls)\n",
        "              for i in range(blocks)])\n",
        "        \n",
        "xresnet18 = partial(XResNet, expansion=1, layers=[2,2,2,2])\n",
        "xresnet50 = partial(XResNet, expansion=4, layers=[3,4,6,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXj4chBgB-OE",
        "colab_type": "text"
      },
      "source": [
        "Let's instantiate the new model and run our check from before to compare with the v1 model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiXKnEKT_tJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_v2b = partial(xresnet18, c_out=10, sa=1, sym=0, act_cls=(lambda: mish))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY8OQr3y_tEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a307fd2f-5cc8-4bfa-e6ac-7e4bd4b3348d"
      },
      "source": [
        "types_to_ignore.add(XResNet)\n",
        "\n",
        "s1 = set(repr(x) for x in filtered_modules(model_v1()))\n",
        "s2 = set(repr(x) for x in filtered_modules(model_v2b()))\n",
        "s1 ^ s2"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2-cKU7MCrUn",
        "colab_type": "text"
      },
      "source": [
        "Great. Next we'd like to check that the forward computation of the two models ties out. The difficulty with this is that initialisation calls random number generators and even if we fix random seeds beforehand, any difference in the sequence of random calls for the two models will lead to divergence.\n",
        "\n",
        "We can check that other details of the forward computation agree by attempting to set the same initialisation for both models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uw_sRAxEXK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reset_dummy_init(model, seed=123):\n",
        "    for m in filtered_modules(model):\n",
        "        if hasattr(m, 'reset_parameters'):\n",
        "            torch.manual_seed(seed)\n",
        "            m.reset_parameters()\n",
        "    return model\n",
        "\n",
        "def compare_fwd(model1, model2):\n",
        "    random_batch = torch.randn(bs,3,size,size, device=device)\n",
        "    assert np.allclose(\n",
        "        model1.to(device)(random_batch).detach().cpu().numpy(), \n",
        "        model2.to(device)(random_batch).detach().cpu().numpy()\n",
        "    )\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyJEPeBlIHXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "918ceeb8-a027-4c35-9773-5ead912b2812"
      },
      "source": [
        "compare_fwd(\n",
        "    reset_dummy_init(model_v1()),\n",
        "    reset_dummy_init(model_v2b()),\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqE5GfH5M_vK",
        "colab_type": "text"
      },
      "source": [
        "This is promising. It remains to check that the initialisations of the two models agree. In general this can be somewhat tricky for the reasons given above. The current situation is actually much nicer as both models are initialised with a final call to an `init_cnn` function and if we fix the random seed before this call we get agreement:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXK2ZhWZWwgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e3ae1f7-ae7e-468b-8a48-4b8c407bd511"
      },
      "source": [
        "model1 = model_v1()\n",
        "torch.manual_seed(123)\n",
        "mxresnet.init_cnn(model1)\n",
        "\n",
        "model2 = model_v2b()\n",
        "torch.manual_seed(123)\n",
        "local.vision.models.xresnet.init_cnn(model2)\n",
        "\n",
        "compare_fwd(model1, model2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivWSZvNSbGK8",
        "colab_type": "text"
      },
      "source": [
        "### Fastai v2 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi7bhGeBm5mc",
        "colab_type": "text"
      },
      "source": [
        "Here is a first attempt at training using the v2 model and codebase + the DALI dataloader we used above. We will use the ranger optimiser from the v1 codebase for now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPi75pwkftVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import local.callback.all #need to import this to patch local.basics.Learner with .to_fp16() method\n",
        "RangerWrapper = lambda *args, **kwargs: local.basics.OptimWrapper(ranger.Ranger(*args, **kwargs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG-Gdi5qak4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1582d17f-388c-4fcf-ee53-74b8eedc4b9d"
      },
      "source": [
        "dali_data = local.basics.DataBunch(train_dl(), valid_dl())\n",
        "\n",
        "learn = local.basics.Learner(\n",
        "            dali_data, \n",
        "            xresnet18(c_out=10, sa=1, sym=0, act_cls=(lambda: mish)), \n",
        "            metrics=[local.basics.accuracy],\n",
        "            loss_func=local.basics.LabelSmoothingCrossEntropy(), \n",
        "            lr=4e-3,\n",
        "            opt_func=partial(RangerWrapper, betas=(0.95, 0.99), eps=1e-6)\n",
        "        ).to_fp16() \n",
        "learn.fit_flat_cos(5, pct_start=0.72)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.465373</td>\n",
              "      <td>1.252990</td>\n",
              "      <td>0.707000</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.199134</td>\n",
              "      <td>1.123996</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.107051</td>\n",
              "      <td>1.133414</td>\n",
              "      <td>0.758000</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.030232</td>\n",
              "      <td>0.959145</td>\n",
              "      <td>0.826000</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.917346</td>\n",
              "      <td>0.838301</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBHVophoeAv",
        "colab_type": "text"
      },
      "source": [
        "To be continued..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPqTP4nZakvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}